{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "785aef1c",
   "metadata": {},
   "source": [
    "# PCA for Feature Engineering\n",
    "\n",
    "There are two ways you could use PCA for feature engineering.\n",
    "\n",
    "The first way is to use it as a descriptive technique. Since the components tell you about the variation, you could compute the MI scores for the components and see what kind of variation is most predictive of your target. That could give you ideas for kinds of features to create -- a product of 'Height' and 'Diameter' if 'Size' is important, say, or a ratio of 'Height' and 'Diameter' if Shape is important. You could even try clustering on one or more of the high-scoring components.\n",
    "\n",
    "The second way is to use the components themselves as features. Because the components expose the variational structure of the data directly, they can often be more informative than the original features. Here are some use-cases:\n",
    "\n",
    "- Dimensionality reduction: When your features are highly redundant (multicollinear, specifically), PCA will partition out the redundancy into one or more near-zero variance components, which you can then drop since they will contain little or no information.\n",
    "\n",
    "- Anomaly detection: Unusual variation, not apparent from the original features, will often show up in the low-variance components. These components could be highly informative in an anomaly or outlier detection task.\n",
    "\n",
    "- Noise reduction: A collection of sensor readings will often share some common background noise. PCA can sometimes collect the (informative) signal into a smaller number of features while leaving the noise alone, thus boosting the signal-to-noise ratio.\n",
    "\n",
    "Decorrelation: Some ML algorithms struggle with highly-correlated features. PCA transforms correlated features into uncorrelated components, which could be easier for your algorithm to work with."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383a992f",
   "metadata": {},
   "source": [
    "# PCA Best Practices\n",
    "There are a few things to keep in mind when applying PCA:\n",
    "\n",
    "- PCA only works with numeric features, like continuous quantities or counts.\n",
    "- PCA is sensitive to scale. It's good practice to standardize your data before applying PCA, unless you know you have good reason not to.\n",
    "- Consider removing or constraining outliers, since they can have an undue influence on the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e24353",
   "metadata": {},
   "source": [
    "# When/Why to use PCA\n",
    "\n",
    "- PCA technique is particularly useful in processing data where multi-colinearity exists between the features/variables.\n",
    "\n",
    "- PCA can be used when the dimensions of the input features are high (e.g. a lot of variables).\n",
    "\n",
    "- PCA can be also used for denoising and data compression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef199f91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
